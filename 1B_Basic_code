import os
import json
import re
import statistics
from datetime import datetime
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util
from pdf2image import convert_from_path
from your_1a_code import extract_pdf_spans, classify_headers, group_adjacent_spans
import spacy
import unicodedata

def normalize_text(text):
    replacements = {
        "\u2013": "-", "\u2014": "-", "\u2018": "'", "\u2019": "'",
        "\u201c": '"', "\u201d": '"', "\u2022": "-", "\u00a0": " ",
        "\u2212": "-", "\u2044": "/"
    }
    for bad, good in replacements.items():
        text = text.replace(bad, good)
    return unicodedata.normalize("NFKC", text)

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

def extract_pos_info(role: str, task: str):
    text = f"{role}. {task}"
    doc = nlp(text)
    pos_tokens = {
        token.lemma_.lower()
        for token in doc
        if token.pos_ in {"NOUN", "ADJ", "VERB", "PROPN"}
           and token.is_alpha and not token.is_stop and len(token) > 3
    }
    entities = {
        ent.text.lower()
        for ent in doc.ents if len(ent.text) > 2
    }
    return list(pos_tokens | entities)

def extract_boost_terms(text):
    stopwords = {
        "the", "and", "or", "to", "for", "a", "of", "in", "on", "at", "with",
        "is", "as", "by", "an", "be", "will", "this", "that", "from"
    }
    words = re.findall(r"[a-zA-Z]+", text.lower())
    return [w for w in words if len(w) > 3 and w not in stopwords]

class PersonaRanker:
    def __init__(self, role, job):
        self.persona_query = f"{role}. {job}"
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.query_embedding = self.model.encode(self.persona_query, convert_to_tensor=True)
        self.boost_terms = extract_boost_terms(self.persona_query)
        print(f"ðŸ” Persona-driven boost terms: {self.boost_terms}")

    def score_section(self, title, content):
        combined = f"{title}. {content}"
        section_embedding = self.model.encode(combined, convert_to_tensor=True)
        base_score = util.pytorch_cos_sim(self.query_embedding, section_embedding).item()
        boost = sum(1 for word in self.boost_terms if word in content.lower()) * 0.01
        if ":" in title and len(title.split()) > 5:
            base_score -= 0.02
        return base_score + boost

def match_heading(span_group, headers):
    group_top = span_group[0]['y_top']
    group_page = span_group[0]['page']
    same_page = [h for h in headers if h['page'] == group_page]
    above = [h for h in same_page if h.get('y_top', 0) < group_top]
    if not above:
        return "General"
    return sorted(above, key=lambda h: group_top - h.get('y_top', 0))[0]['text']

def extract_sections_for_ranking(pdf_path, spans, images, max_size_th, mid_size, max_size):
    headers = classify_headers(spans, images, max_size_th, mid_size, max_size)
    flat_spans = [s for page in spans for s in page]
    paragraphs = group_adjacent_spans(flat_spans)

    sections = []
    for para in paragraphs:
        text = normalize_text(" ".join([s["text"] for s in para]))
        if 15 < len(text) < 1500:
            section_title = match_heading(para, headers)
            if section_title.startswith(("â€¢", "-", "â€“")) or len(section_title.split()) > 10:
                section_title = text.split(".")[0][:80]
            sections.append({
                "document": os.path.basename(pdf_path),
                "section_title": section_title,
                "content": text,
                "page": para[0]["page"]
            })
    return sections

def generate_round1b_output(input_json_path, pdf_dir,sections_to_merge=5):
    top_pdf_count=5
    with open(input_json_path) as f:
        config = json.load(f)

    role = config["persona"]["role"]
    task = config["job_to_be_done"]["task"]
    documents = config["documents"]
    ranker = PersonaRanker(role, task)
    keyword_list = extract_pos_info(role, task)
    print(f"\U0001f9e0 Extracted keywords & facts: {keyword_list}")

    pdf_section_info = []

    for doc in documents:
        filename = doc["filename"]
        pdf_path = os.path.join(pdf_dir, filename)
        print(f"\U0001f4c4 Processing {filename}...")

        spans, threshold_size, doc_obj = extract_pdf_spans(pdf_path)
        images = convert_from_path(pdf_path, dpi=200)

        all_sizes = sorted({s["font_size"] for page in spans for s in page}, reverse=True)
        max_size_th = all_sizes[1] if len(all_sizes) > 1 else threshold_size
        mid_size = statistics.median(all_sizes) if len(all_sizes) > 1 else max_size_th * 0.85
        max_size = all_sizes[0] if all_sizes else threshold_size

        all_sections = extract_sections_for_ranking(pdf_path, spans, images, max_size_th, mid_size, max_size)
        print(f"   â†³ Found {len(all_sections)} total sections")

        matched_sections = []
        for sec in all_sections:
            if any(k in sec["content"].lower() for k in keyword_list):
                sec["score"] = ranker.score_section(sec["section_title"], sec["content"])
                matched_sections.append(sec)

        # Remove undesired sections
        matched_sections = [
            s for s in matched_sections
            if s["section_title"].strip().lower() not in {"general", "conclusion"}
        ]

        match_count = len(matched_sections)
        print(f"   â†³ {match_count} keyword-matched sections")

        if match_count > 0:
            pdf_section_info.append({
                "filename": filename,
                "match_count": match_count,
                "sections": matched_sections
            })

    # Sort PDFs by keyword match count
    top_pdfs = sorted(pdf_section_info, key=lambda x: x["match_count"], reverse=True)[:top_pdf_count]

    # Pick the best scored section from each of top 5 PDFs
    top_sections = []
    for pdf in top_pdfs:
        best_section = sorted(pdf["sections"], key=lambda x: x["score"], reverse=True)[0]
        top_sections.append(best_section)

    # Add rank
    top_sections = sorted(top_sections, key=lambda x: x["score"], reverse=True)
    for i, sec in enumerate(top_sections):
        sec["importance_rank"] = i + 1

    output = {
        "metadata": {
            "input_documents": [doc["filename"] for doc in documents],
            "persona": role,
            "job_to_be_done": task,
            "processing_timestamp": datetime.utcnow().isoformat()
        },
        "extracted_sections": [
            {
                "document": sec["document"],
                "section_title": sec["section_title"],
                "importance_rank": sec["importance_rank"],
                "page_number": sec["page"]
            } for sec in top_sections
        ],
        "subsection_analysis": [
            {
                "document": sec["document"],
                "refined_text": sec["content"],
                "page_number": sec["page"]
            } for sec in top_sections
        ]
    }

    with open("challenge1b_output.json", "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Selected best 1 section from each of top {top_pdf_count} PDFs by keyword matches.")
    print("âœ… challenge1b_output.json saved.")
